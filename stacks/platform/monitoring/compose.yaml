services:
  op-secrets:
    image: 1password/op:2@sha256:57d7d6a2bb2b74b2cf8111f6afb2973c74772198f82ea30359a53faae9fff5b1
    deploy:
      mode: replicated-job
      restart_policy:
        condition: none
      resources:
        limits:
          cpus: '0.25'
          memory: 128M
    environment:
      OP_CONNECT_HOST: http://op-connect-api:8080
      OP_CONNECT_TOKEN_FILE: /run/secrets/op_connect_token
    secrets:
      - op_connect_token
    volumes:
      - ./grafana.env.template:/templates/grafana.env.template:ro
      - /mnt/apps01/appdata/monitoring/secrets:/secrets
    networks:
      - op-connect
    command: >
      sh -ec "
      export OP_CONNECT_TOKEN=$$(cat /run/secrets/op_connect_token) &&
      if op inject -i /templates/grafana.env.template -o /secrets/grafana.env -f; then
        chmod 644 /secrets/grafana.env &&
        echo 'Monitoring secrets injected successfully';
      elif [ -s /secrets/grafana.env ]; then
        echo 'Monitoring secret injection failed; using existing /secrets/grafana.env';
      else
        echo 'Monitoring secret injection failed and no existing /secrets/grafana.env present' >&2;
        exit 1;
      fi
      "

  # Pinned to barbary — TSDB requires local storage
  prometheus:
    image: prom/prometheus:v3.5.1
    deploy:
      replicas: 1
      placement:
        constraints:
          - node.hostname == barbary
      restart_policy:
        condition: any
        delay: 5s
        max_attempts: 3
      resources:
        limits:
          cpus: '1'
          memory: 1024M
        reservations:
          cpus: '0.5'
          memory: 256M
      labels:
        homepage.group: "Monitoring"
        homepage.name: "Prometheus"
        homepage.icon: "prometheus.png"
        homepage.href: "https://prometheus.in.hypyr.space"
        homepage.description: "Metrics collection"
        homepage.widget.type: "prometheus"
        homepage.widget.url: "http://prometheus:9090"

        caddy: prometheus.in.hypyr.space
        caddy.reverse_proxy: "{{upstreams 9090}}"
        caddy.tls.dns: "cloudflare {env.CLOUDFLARE_API_TOKEN}"
        caddy.forward_auth: "http://authentik-server:9000"
        caddy.forward_auth.uri: "/outpost.goauthentik.io/auth/caddy"
        caddy.forward_auth.copy_headers: "X-Authentik-Username X-Authentik-Groups X-Authentik-Email X-Authentik-Name X-Authentik-Uid"

        kuma.prometheus.http.name: "Prometheus"
        kuma.prometheus.http.url: "http://prometheus:9090/-/healthy"
        kuma.prometheus.http.interval: "60"
        kuma.prometheus.http.maxretries: "3"
        kuma.prometheus.http.accepted_statuscodes: '["200-299"]'
    command:
      - --config.file=/etc/prometheus/prometheus.yml
      - --storage.tsdb.path=/prometheus
      - --storage.tsdb.retention.time=15d
      - --web.enable-lifecycle
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - /mnt/data01/appdata/monitoring/prometheus:/prometheus
    networks:
      - monitoring
      - proxy_network

  # Pinned to barbary — chunk storage requires local disk
  loki:
    image: grafana/loki:3.6.5
    deploy:
      replicas: 1
      placement:
        constraints:
          - node.hostname == barbary
      restart_policy:
        condition: any
        delay: 5s
        max_attempts: 3
      resources:
        limits:
          cpus: '1'
          memory: 1024M
        reservations:
          cpus: '0.25'
          memory: 256M
      labels:
        caddy: loki.in.hypyr.space
        caddy.reverse_proxy: "{{upstreams 3100}}"
        caddy.tls.dns: "cloudflare {env.CLOUDFLARE_API_TOKEN}"
        caddy.forward_auth: "http://authentik-server:9000"
        caddy.forward_auth.uri: "/outpost.goauthentik.io/auth/caddy"
        caddy.forward_auth.copy_headers: "X-Authentik-Username X-Authentik-Groups X-Authentik-Email X-Authentik-Name X-Authentik-Uid"

        kuma.loki.http.name: "Loki"
        kuma.loki.http.url: "http://loki:3100/ready"
        kuma.loki.http.interval: "60"
        kuma.loki.http.maxretries: "3"
        kuma.loki.http.accepted_statuscodes: '["200-299"]'
    command: -config.file=/etc/loki/loki-config.yaml
    volumes:
      - ./loki/loki-config.yaml:/etc/loki/loki-config.yaml:ro
      - /mnt/data01/appdata/monitoring/loki:/loki
    networks:
      - monitoring
      - proxy_network

  # Floats freely — state in PostgreSQL
  grafana:
    image: grafana/grafana:12.3.3
    user: "472:999"
    deploy:
      replicas: 1
      restart_policy:
        condition: any
        delay: 5s
        max_attempts: 3
      resources:
        limits:
          cpus: '1'
          memory: 1024M
        reservations:
          cpus: '0.5'
          memory: 512M
      labels:
        homepage.group: "Monitoring"
        homepage.name: "Grafana"
        homepage.icon: "grafana.png"
        homepage.href: "https://grafana.in.hypyr.space"
        homepage.description: "Metrics and logs dashboards"
        homepage.widget.type: "grafana"
        homepage.widget.url: "https://grafana.in.hypyr.space"
        homepage.widget.username: "{{HOMEPAGE_VAR_GRAFANA_USER}}"
        homepage.widget.password: "{{HOMEPAGE_VAR_GRAFANA_PASSWORD}}"

        caddy: grafana.in.hypyr.space
        caddy.reverse_proxy: "{{upstreams 3000}}"
        caddy.tls.dns: "cloudflare {env.CLOUDFLARE_API_TOKEN}"
        caddy.forward_auth: "http://authentik-server:9000"
        caddy.forward_auth.uri: "/outpost.goauthentik.io/auth/caddy"
        caddy.forward_auth.copy_headers: "X-Authentik-Username X-Authentik-Groups X-Authentik-Email X-Authentik-Name X-Authentik-Uid"

        kuma.grafana.http.name: "Grafana"
        kuma.grafana.http.url: "https://grafana.in.hypyr.space/api/health"
        kuma.grafana.http.interval: "60"
        kuma.grafana.http.maxretries: "3"
        kuma.grafana.http.accepted_statuscodes: '["200-299"]'
    environment:
      GF_PATHS_PROVISIONING: /etc/grafana/provisioning
      GF_SERVER_ROOT_URL: https://grafana.in.hypyr.space
      GF_SECURITY_DISABLE_GRAVATAR: "true"
      GF_USERS_ALLOW_SIGN_UP: "false"
      GF_DATABASE_TYPE: postgres
      GF_DATABASE_HOST: postgresql:5432
      GF_DATABASE_NAME: grafana
      GF_DATABASE_SSL_MODE: disable
    volumes:
      - /mnt/data01/appdata/monitoring/grafana:/var/lib/grafana
      - /mnt/apps01/appdata/monitoring/secrets:/run/monitoring-secrets:ro
      - ./grafana/provisioning:/etc/grafana/provisioning:ro
    networks:
      - monitoring
      - proxy_network
      - postgres
    entrypoint: ["/bin/sh", "-c"]
    command: >
      "set -a;
       if [ -f /run/monitoring-secrets/grafana.env ]; then
         . /run/monitoring-secrets/grafana.env;
       fi;
       set +a;
       exec /run.sh"

  node-exporter:
    image: prom/node-exporter:v1.10.2
    deploy:
      mode: global
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
    command:
      - --path.procfs=/host/proc
      - --path.sysfs=/host/sys
      - --path.rootfs=/host/rootfs
      - --collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/host/rootfs:ro
    networks:
      - monitoring

  cadvisor:
    image: ghcr.io/google/cadvisor:0.56.2
    deploy:
      mode: global
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
    command:
      - --docker_only
      - --housekeeping_interval=30s
      - --disable_metrics=cpu_topology,hugetlb,memory_numa,percpu,process,referenced_memory,resctrl,sched,tcp,udp
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /sys:/sys:ro
      - /mnt/.ix-apps/docker:/var/lib/docker:ro
    networks:
      - monitoring

  alloy:
    image: grafana/alloy:v1.13.1
    deploy:
      mode: global
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
    command:
      - run
      - /etc/alloy/config.alloy
      - --storage.path=/var/lib/alloy/data
    volumes:
      - ./alloy/config.alloy:/etc/alloy/config.alloy:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /mnt/.ix-apps/docker/containers:/var/lib/docker/containers:ro
      - /mnt/data01/appdata/monitoring/alloy-data:/var/lib/alloy/data
    networks:
      - monitoring

networks:
  monitoring:
    driver: overlay
    attachable: true
  proxy_network:
    external: true
  postgres:
    name: platform_postgres_postgres
    external: true
  op-connect:
    name: op-connect_op-connect
    external: true

secrets:
  op_connect_token:
    external: true
