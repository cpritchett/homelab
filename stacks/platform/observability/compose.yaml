services:
  op-secrets:
    image: 1password/op:2@sha256:4ac3658f1e91a27ec9503f2134d4b2b77769b36d35b2e370ac89b5976573a17b
    deploy:
      mode: replicated-job
      restart_policy:
        condition: none
      resources:
        limits:
          cpus: '0.25'
          memory: 128M
    environment:
      OP_CONNECT_HOST: http://op-connect-api:8080
      OP_CONNECT_TOKEN_FILE: /run/secrets/op_connect_token
    secrets:
      - op_connect_token
    volumes:
      - ./homepage/homepage.env.template:/templates/homepage.env.template:ro
      - /mnt/apps01/appdata/homepage/secrets:/secrets
    networks:
      - op-connect
    command: >
      sh -ec "
      export OP_CONNECT_TOKEN=$$(cat /run/secrets/op_connect_token) &&
      if op inject -i /templates/homepage.env.template -o /secrets/homepage.env -f; then
        chmod 600 /secrets/homepage.env &&
        echo 'Homepage secrets injected successfully';
      elif [ -s /secrets/homepage.env ]; then
        echo 'Homepage secret injection failed; using existing /secrets/homepage.env';
      else
        echo 'Homepage secret injection failed and no existing /secrets/homepage.env present' >&2;
        exit 1;
      fi
      "

  # Homepage Dashboard
  homepage:
    image: ghcr.io/gethomepage/homepage:v1.10.1
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 512M
      labels:
        # Caddy ingress
        caddy: home.in.hypyr.space
        caddy.reverse_proxy: "{{upstreams 3000}}"
        caddy.tls.dns: "cloudflare {env.CLOUDFLARE_API_TOKEN}"
        caddy.forward_auth: "http://authentik-server:9000"
        caddy.forward_auth.uri: "/outpost.goauthentik.io/auth/caddy"
        caddy.forward_auth.copy_headers: "X-Authentik-Username X-Authentik-Groups X-Authentik-Email X-Authentik-Name X-Authentik-Uid"
        # AutoKuma monitoring
        kuma.homepage.http.name: "Homepage Dashboard"
        kuma.homepage.http.url: "http://homepage:3000"
        kuma.homepage.http.interval: "60"
        kuma.homepage.http.maxretries: "3"
        kuma.homepage.http.accepted_statuscodes: '["200-299"]'
    environment:
      TZ: America/Chicago
      HOMEPAGE_ALLOWED_HOSTS: home.in.hypyr.space
      HOMEPAGE_VAR_DOCKER_HOST: tcp://docker-socket-proxy:2375
    networks:
      - proxy_network
      - observability_internal
    volumes:
      - ./homepage/config:/app/config
      - /mnt/apps01/appdata/homepage/icons:/app/public/icons
      - /mnt/apps01/appdata/homepage/images:/app/public/images
      - /mnt/apps01/appdata/homepage/secrets:/run/homepage-secrets:ro
    entrypoint: ["/bin/sh", "-c"]
    command: >
      "set -a;
       if [ -f /run/homepage-secrets/homepage.env ]; then
         . /run/homepage-secrets/homepage.env;
       fi;
       set +a;
       exec docker-entrypoint.sh node server.js"
    healthcheck:
      disable: true

  docker-socket-proxy:
    image: ghcr.io/tecnativa/docker-socket-proxy:latest
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
    environment:
      # Minimal permissions for Homepage
      CONTAINERS: '1'
      SERVICES: '1'
      TASKS: '1'
      NETWORKS: '1'
      NODES: '1'
      INFO: '1'
      VERSION: '1'
      PING: '1'
    networks:
      - observability_internal
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
    healthcheck:
      test: ["CMD", "wget", "-qO-", "http://localhost:2375/_ping"]
      interval: 10s
      timeout: 3s
      retries: 3

  # Uptime Kuma Monitoring
  # Pinned to barbary â€” SQLite DB, no postgres support
  uptime-kuma:
    image: louislam/uptime-kuma:2.0.2@sha256:4c364ef96aaddac7ec4c85f5e5f31c3394d35f631381ccbbf93f18fd26ac7cba
    deploy:
      replicas: 1
      placement:
        constraints:
          - node.hostname == barbary
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
      resources:
        limits:
          cpus: '1'
          memory: 512M
        reservations:
          cpus: '0.5'
          memory: 256M
      labels:
        # Caddy ingress
        caddy: status.in.hypyr.space
        caddy.reverse_proxy: "{{upstreams 3001}}"
        caddy.tls.dns: "cloudflare {env.CLOUDFLARE_API_TOKEN}"
        # Homepage registration
        homepage.group: "Observability"
        homepage.name: "Uptime Kuma"
        homepage.icon: "uptime-kuma.png"
        homepage.href: "https://status.in.hypyr.space"
        homepage.description: "Uptime monitoring"
        homepage.widget.type: "uptimekuma"
        homepage.widget.url: "http://uptime-kuma:3001"
        # AutoKuma self-monitoring
        kuma.uptime-kuma.http.name: "Uptime Kuma"
        kuma.uptime-kuma.http.url: "https://status.in.hypyr.space"
        kuma.uptime-kuma.http.interval: "60"
        kuma.uptime-kuma.http.maxretries: "3"
        kuma.uptime-kuma.http.accepted_statuscodes: '["200-299"]'
    volumes:
      - /mnt/apps01/appdata/uptime-kuma:/app/data
    networks:
      proxy_network:
        aliases:
          - uptime-kuma
      observability_internal:

  # AutoKuma - Auto-create monitors from Docker labels
  autokuma:
    image: ghcr.io/bigboot/autokuma:2.0.0
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
    configs:
      - source: autokuma_wrapper
        target: /autokuma-wrapper.sh
        mode: 0755
    entrypoint: ["/bin/sh", "/autokuma-wrapper.sh"]
    environment:
      # Uptime Kuma connection
      AUTOKUMA__KUMA__URL: http://uptime-kuma:3001
      # Docker provider configuration
      AUTOKUMA__DOCKER__HOSTS: tcp://docker-socket-proxy:2375
      AUTOKUMA__DOCKER__SOURCE: Services
      # Label prefix (e.g., kuma.example.http.url)
      AUTOKUMA__DOCKER__LABEL_PREFIX: kuma
      # Sync settings
      AUTOKUMA__SYNC_INTERVAL: 60
      # Tag all auto-created monitors
      AUTOKUMA__TAG: autokuma
      AUTOKUMA__TAG_NAME: Source
      AUTOKUMA__TAG_COLOR: "#42C0FB"
    secrets:
      - uptime_kuma_username
      - uptime_kuma_password
    networks:
      - observability_internal
    depends_on:
      - uptime-kuma
      - docker-socket-proxy

networks:
  proxy_network:
    external: true
  observability_internal:
    driver: overlay
  op-connect:
    name: op-connect_op-connect
    external: true

configs:
  autokuma_wrapper:
    file: ./autokuma-wrapper.sh

secrets:
  uptime_kuma_username:
    external: true
  uptime_kuma_password:
    external: true
  op_connect_token:
    external: true
