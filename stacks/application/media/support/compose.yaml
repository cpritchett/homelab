###############################################################################
# Media Stack — Supporting Services
#
# Services: Bazarr, Tautulli, Maintainerr, Recyclarr, Seerr, Wizarr, Tracearr
# Tier: Application (depends on Infrastructure + Platform + Media Core)
# Spec: specs/007-media-stack-migration
# ADR: ADR-0033 Phase 3, ADR-0034 (labels), ADR-0035 (secrets)
###############################################################################

services:
  # ── Secret Hydration (one-shot job) ──────────────────────────
  op-secrets:
    image: 1password/op:2
    deploy:
      mode: replicated-job
      restart_policy:
        condition: none
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
    environment:
      OP_CONNECT_HOST: http://op-connect-api:8080
      OP_CONNECT_TOKEN_FILE: /run/secrets/op_connect_token
    secrets:
      - op_connect_token
    volumes:
      - ./support.env.template:/templates/support.template:ro
      - /mnt/apps01/appdata/media/support/secrets:/secrets
    networks:
      - op-connect
    command: >
      sh -ec "
      export OP_CONNECT_TOKEN=$$(cat /run/secrets/op_connect_token) &&
      op inject -i /templates/support.template -o /secrets/support.env -f &&
      chmod 644 /secrets/support.env &&
      echo 'Secrets injected successfully'
      "

  # ── Recyclarr ────────────────────────────────────────────────
  # Syncs TRaSH Guides quality profiles to Sonarr/Radarr (one-shot per deploy)
  recyclarr:
    image: ghcr.io/recyclarr/recyclarr:latest@sha256:30e13877e8ef2242b053b986e69e64801797b39ae4f74b744d8f4dc3f98757ab
    deploy:
      mode: replicated-job
      restart_policy:
        condition: none
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
    user: "1701:1702"
    volumes:
      - /mnt/apps01/appdata/media/recyclarr/config:/config
    networks:
      - proxy_network
    command: sync

  # ── Bazarr ───────────────────────────────────────────────────
  # Pinned to barbary — SQLite DB, no postgres support
  bazarr:
    image: lscr.io/linuxserver/bazarr:latest@sha256:1cf40186b1bc35bec87f4e4892d5d8c06086da331010be03e3459a86869c5e74
    deploy:
      replicas: 1
      placement:
        constraints:
          - node.hostname == barbary
      restart_policy:
        condition: any
        delay: 5s
        max_attempts: 3
      resources:
        limits:
          cpus: '1'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
      labels:
        # Homepage: Media dashboard
        homepage.group: "Media"
        homepage.name: "Bazarr"
        homepage.icon: "bazarr.png"
        homepage.href: "https://bazarr.in.hypyr.space"
        homepage.description: "Subtitle management"
        homepage.widget.type: "bazarr"
        homepage.widget.url: "http://bazarr:6767"

        # Caddy: Reverse proxy + Authentik forward auth
        caddy: bazarr.in.hypyr.space
        caddy.reverse_proxy: "{{upstreams 6767}}"
        caddy.tls.dns: "cloudflare {env.CLOUDFLARE_API_TOKEN}"
        caddy.forward_auth: "http://authentik-server:9000"
        caddy.forward_auth.uri: "/outpost.goauthentik.io/auth/caddy"
        caddy.forward_auth.copy_headers: "X-Authentik-Username X-Authentik-Groups X-Authentik-Email X-Authentik-Name X-Authentik-Uid"

        # AutoKuma: Uptime monitoring
        kuma.bazarr.http.name: "Bazarr"
        kuma.bazarr.http.url: "https://bazarr.in.hypyr.space/ping"
        kuma.bazarr.http.interval: "60"
        kuma.bazarr.http.maxretries: "3"
    environment:
      PUID: "1701"
      PGID: "1702"
      TZ: "America/Chicago"
    volumes:
      - /mnt/apps01/appdata/media/bazarr/config:/config
      - /mnt/data01/data:/data
    networks:
      - media_support
      - proxy_network

  # ── Tautulli ─────────────────────────────────────────────────
  # Pinned to barbary — SQLite DB, Plex-adjacent
  tautulli:
    image: lscr.io/linuxserver/tautulli:latest
    deploy:
      replicas: 1
      placement:
        constraints:
          - node.hostname == barbary
      restart_policy:
        condition: any
        delay: 5s
        max_attempts: 3
      resources:
        limits:
          cpus: '1'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
      labels:
        # Homepage: Media dashboard
        homepage.group: "Media"
        homepage.name: "Tautulli"
        homepage.icon: "tautulli.png"
        homepage.href: "https://tautulli.in.hypyr.space"
        homepage.description: "Plex monitoring & analytics"
        homepage.widget.type: "tautulli"
        homepage.widget.url: "http://tautulli:8181"

        # Caddy: Reverse proxy + Authentik forward auth
        caddy: tautulli.in.hypyr.space
        caddy.reverse_proxy: "{{upstreams 8181}}"
        caddy.tls.dns: "cloudflare {env.CLOUDFLARE_API_TOKEN}"
        caddy.forward_auth: "http://authentik-server:9000"
        caddy.forward_auth.uri: "/outpost.goauthentik.io/auth/caddy"
        caddy.forward_auth.copy_headers: "X-Authentik-Username X-Authentik-Groups X-Authentik-Email X-Authentik-Name X-Authentik-Uid"

        # AutoKuma: Uptime monitoring
        kuma.tautulli.http.name: "Tautulli"
        kuma.tautulli.http.url: "https://tautulli.in.hypyr.space/status"
        kuma.tautulli.http.interval: "60"
        kuma.tautulli.http.maxretries: "3"
    environment:
      PUID: "1701"
      PGID: "1702"
      TZ: "America/Chicago"
    volumes:
      - /mnt/apps01/appdata/media/tautulli/config:/config
    networks:
      - media_support
      - proxy_network

  # ── Maintainerr ──────────────────────────────────────────────
  # Pinned to barbary — SQLite DB
  maintainerr:
    image: ghcr.io/jorenn92/maintainerr:latest
    user: "1701:1702"
    deploy:
      replicas: 1
      placement:
        constraints:
          - node.hostname == barbary
      restart_policy:
        condition: any
        delay: 5s
        max_attempts: 3
      resources:
        limits:
          cpus: '1'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
      labels:
        # Homepage: Media dashboard
        homepage.group: "Media"
        homepage.name: "Maintainerr"
        homepage.icon: "maintainerr.png"
        homepage.href: "https://maintainerr.in.hypyr.space"
        homepage.description: "Plex library maintenance"

        # Caddy: Reverse proxy + Authentik forward auth
        caddy: maintainerr.in.hypyr.space
        caddy.reverse_proxy: "{{upstreams 6246}}"
        caddy.tls.dns: "cloudflare {env.CLOUDFLARE_API_TOKEN}"
        caddy.forward_auth: "http://authentik-server:9000"
        caddy.forward_auth.uri: "/outpost.goauthentik.io/auth/caddy"
        caddy.forward_auth.copy_headers: "X-Authentik-Username X-Authentik-Groups X-Authentik-Email X-Authentik-Name X-Authentik-Uid"

        # AutoKuma: Uptime monitoring
        kuma.maintainerr.http.name: "Maintainerr"
        kuma.maintainerr.http.url: "https://maintainerr.in.hypyr.space"
        kuma.maintainerr.http.interval: "60"
        kuma.maintainerr.http.maxretries: "3"
        kuma.maintainerr.http.accepted_statuscodes: '["200-299","301","302"]'
    volumes:
      - /mnt/apps01/appdata/media/maintainerr/config:/opt/data
    networks:
      - media_support
      - proxy_network

  # ── Seerr ────────────────────────────────────────────────────
  # Floats freely — state in PostgreSQL
  # Media request management (formerly Jellyseerr)
  seerr:
    image: ghcr.io/seerr-team/seerr:v3.0.0
    init: true
    user: "1000:999"
    deploy:
      replicas: 1
      restart_policy:
        condition: any
        delay: 5s
      resources:
        limits:
          cpus: '1'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
      labels:
        # Homepage: Media dashboard
        homepage.group: "Media"
        homepage.name: "Seerr"
        homepage.icon: "seerr.png"
        homepage.href: "https://requests.in.hypyr.space"
        homepage.description: "Media requests"
        homepage.widget.type: "overseerr"
        homepage.widget.url: "http://seerr:5055"

        # Caddy: Reverse proxy (NO forward auth — Seerr has own auth via Plex/Jellyfin)
        caddy: requests.in.hypyr.space
        caddy.reverse_proxy: "{{upstreams 5055}}"
        caddy.tls.dns: "cloudflare {env.CLOUDFLARE_API_TOKEN}"

        # AutoKuma: Uptime monitoring
        kuma.seerr.http.name: "Seerr"
        kuma.seerr.http.url: "https://requests.in.hypyr.space"
        kuma.seerr.http.interval: "60"
        kuma.seerr.http.maxretries: "3"
        kuma.seerr.http.accepted_statuscodes: '["200-299","301","302"]'
    environment:
      DB_TYPE: postgres
      DB_HOST: postgresql
      DB_PORT: "5432"
      DB_NAME: seerr
    volumes:
      - /mnt/apps01/appdata/media/seerr/config:/app/config
      - /mnt/apps01/appdata/media/support/secrets:/secrets:ro
    networks:
      - media_support
      - proxy_network
      - postgres
    entrypoint: ["/bin/sh", "-c"]
    command: >
      "set -a && [ -f /secrets/support.env ] && . /secrets/support.env && set +a &&
       exec npm start"

  # ── Wizarr ──────────────────────────────────────────────────
  # Pinned to barbary — SQLite DB
  # User invitation & onboarding for Plex/Jellyfin
  wizarr:
    image: ghcr.io/wizarrrr/wizarr:latest@sha256:83fc0b50985d196226b7c28a27062a1c4e63124a5d6d2448710aee222637b79e
    deploy:
      replicas: 1
      placement:
        constraints:
          - node.hostname == barbary
      restart_policy:
        condition: any
        delay: 5s
        max_attempts: 3
      resources:
        limits:
          cpus: '1'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
      labels:
        # Homepage: Media dashboard
        homepage.group: "Media"
        homepage.name: "Wizarr"
        homepage.icon: "wizarr.png"
        homepage.href: "https://invite.in.hypyr.space"
        homepage.description: "User invitations"

        # Caddy: Reverse proxy (no forward auth — invite links must be public)
        caddy: invite.in.hypyr.space
        caddy.reverse_proxy: "{{upstreams 5690}}"
        caddy.tls.dns: "cloudflare {env.CLOUDFLARE_API_TOKEN}"

        # AutoKuma: Uptime monitoring
        kuma.wizarr.http.name: "Wizarr"
        kuma.wizarr.http.url: "https://invite.in.hypyr.space"
        kuma.wizarr.http.interval: "60"
        kuma.wizarr.http.maxretries: "3"
        kuma.wizarr.http.accepted_statuscodes: '["200-299","301","302"]'
    environment:
      TZ: "America/Chicago"
    volumes:
      - /mnt/apps01/appdata/media/wizarr/database:/data/database
    networks:
      - media_support
      - proxy_network

  # ── Tracearr TimescaleDB ─────────────────────────────────────
  # Pinned to barbary — ZFS-backed database
  tracearr-timescaledb:
    image: timescale/timescaledb:latest-pg17@sha256:f5f9f09d57799d237f481e5b9ad517eae5088fed3dbb487e2138419e6196fc78
    deploy:
      replicas: 1
      placement:
        constraints:
          - node.hostname == barbary
      restart_policy:
        condition: any
        delay: 5s
        max_attempts: 3
      resources:
        limits:
          cpus: '2'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
    volumes:
      - /mnt/apps01/appdata/media/tracearr/timescaledb:/var/lib/postgresql/data
      - /mnt/apps01/appdata/media/support/secrets:/secrets:ro
    networks:
      - media_support
    entrypoint: ["/bin/sh", "-c"]
    command: >
      "set -a && [ -f /secrets/support.env ] && . /secrets/support.env && set +a &&
       export POSTGRES_USER=tracearr &&
       export POSTGRES_DB=tracearr &&
       export POSTGRES_PASSWORD=\"$${TRACEARR_DB_PASSWORD}\" &&
       exec docker-entrypoint.sh postgres
       -c shared_preload_libraries=timescaledb
       -c timescaledb.telemetry_level=off"

  # ── Tracearr Redis ──────────────────────────────────────────
  tracearr-redis:
    image: redis:8-alpine
    deploy:
      replicas: 1
      restart_policy:
        condition: any
        delay: 5s
        max_attempts: 3
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.1'
          memory: 64M
    volumes:
      - /mnt/apps01/appdata/media/tracearr/redis:/data
    networks:
      - media_support
    command: redis-server --appendonly yes

  # ── Tracearr ────────────────────────────────────────────────
  # Unified monitoring for Plex, Jellyfin, Emby
  # Floats freely — state in TimescaleDB + Redis
  tracearr:
    image: ghcr.io/connorgallopo/tracearr:latest@sha256:d5067a395364296abea5478be4ca61f7e99f483af91b49bb0536671796a70125
    user: "1001:999"
    deploy:
      replicas: 1
      restart_policy:
        condition: any
        delay: 5s
        max_attempts: 3
      resources:
        limits:
          cpus: '1'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
      labels:
        # Homepage: Media dashboard
        homepage.group: "Media"
        homepage.name: "Tracearr"
        homepage.icon: "tracearr.png"
        homepage.href: "https://tracearr.in.hypyr.space"
        homepage.description: "Stream monitoring & analytics"

        # Caddy: Reverse proxy + Authentik forward auth
        caddy: tracearr.in.hypyr.space
        caddy.reverse_proxy: "{{upstreams 3000}}"
        caddy.tls.dns: "cloudflare {env.CLOUDFLARE_API_TOKEN}"
        caddy.forward_auth: "http://authentik-server:9000"
        caddy.forward_auth.uri: "/outpost.goauthentik.io/auth/caddy"
        caddy.forward_auth.copy_headers: "X-Authentik-Username X-Authentik-Groups X-Authentik-Email X-Authentik-Name X-Authentik-Uid"

        # AutoKuma: Uptime monitoring
        kuma.tracearr.http.name: "Tracearr"
        kuma.tracearr.http.url: "https://tracearr.in.hypyr.space/health"
        kuma.tracearr.http.interval: "60"
        kuma.tracearr.http.maxretries: "3"
    volumes:
      - /mnt/apps01/appdata/media/tracearr/config:/app/config
      - /mnt/apps01/appdata/media/tracearr/image-cache:/app/image-cache
      - /mnt/apps01/appdata/media/support/secrets:/secrets:ro
    networks:
      - media_support
      - proxy_network
    entrypoint: ["/bin/sh", "-c"]
    command: >
      "set -a && [ -f /secrets/support.env ] && . /secrets/support.env && set +a &&
       export DATABASE_URL=\"postgresql://tracearr:$${TRACEARR_DB_PASSWORD}@tracearr-timescaledb:5432/tracearr\" &&
       export REDIS_URL=\"redis://tracearr-redis:6379\" &&
       export JWT_SECRET=\"$${TRACEARR_JWT_SECRET}\" &&
       export COOKIE_SECRET=\"$${TRACEARR_COOKIE_SECRET}\" &&
       exec docker-entrypoint.sh node apps/server/dist/index.js"

# ── Networks ─────────────────────────────────────────────────
networks:
  media_support:
    driver: overlay
  proxy_network:
    external: true
  postgres:
    name: platform_postgres_postgres
    external: true
  op-connect:
    name: op-connect_op-connect
    external: true

# ── Secrets ──────────────────────────────────────────────────
secrets:
  op_connect_token:
    external: true

