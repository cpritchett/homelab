---
version: "3"

tasks:
  browse-pvc:
    desc: Mount a PVC to a temp container [NS={{.NS}}] [CLAIM=required]
    interactive: true
    cmd: kubectl browse-pvc --namespace {{.NS}} --image docker.io/library/alpine:latest {{.CLAIM}}
    vars:
      NS: '{{.NS | default "default"}}'
    requires:
      vars: [CLAIM]
    preconditions:
      - kubectl --namespace {{.NS}} get persistentvolumeclaims {{.CLAIM}}
      - kubectl browse-pvc --version
      - which kubectl

  node-shell:
    desc: Open a shell to a node [NS={{.NS}}] [NODE=required]
    interactive: true
    cmd: kubectl node-shell -n {{.NS}} -x {{.NODE}}
    vars:
      NS: '{{.NS | default "kube-system"}}'
    requires:
      vars: [NODE]
    preconditions:
      - kubectl get nodes {{.NODE}}
      - kubectl node-shell --version
      - which kubectl

  sync-secrets:
    desc: Force sync all ExternalSecrets (and fix 1Password Connect if needed)
    cmds:
      - echo "üîç Checking 1Password Connect status..."
      - |
        if ! kubectl get clustersecretstore onepassword -o jsonpath='{.status.conditions[0].status}' | grep -q "True"; then
          echo "‚ùå 1Password Connect not working, fixing..."
          op inject -i {{.BOOTSTRAP_DIR}}/resources.yaml | kubectl apply -f -
          kubectl delete pod -l app.kubernetes.io/name=onepassword -n external-secrets
          echo "‚è≥ Waiting for 1Password Connect to restart..."
          kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=onepassword -n external-secrets --timeout=60s
        else
          echo "‚úÖ 1Password Connect is healthy"
        fi
      - echo "üîÑ Force syncing all ExternalSecrets..."
      - for: { var: SECRETS, split: "\n" }
        cmd: kubectl --namespace {{splitList "," .ITEM | first}} annotate externalsecret {{splitList "," .ITEM | last}} force-sync="{{now | unixEpoch}}" --overwrite
      - echo "‚úÖ Secret sync complete"
    vars:
      SECRETS:
        sh: kubectl get externalsecret --all-namespaces --no-headers --output=jsonpath='{range .items[*]}{.metadata.namespace},{.metadata.name}{"\n"}{end}'
    preconditions:
      - op user get --me
      - test -f {{.BOOTSTRAP_DIR}}/resources.yaml
      - which kubectl op

  reconcile:
    desc: Force Flux reconciliation to pull latest Git changes
    cmds:
      - echo "üîÑ Reconciling Git source..."
      - flux reconcile source git flux-system -n flux-system
      - echo "üîÑ Reconciling cluster applications..."
      - flux reconcile kustomization apps -n flux-system
      - echo "‚úÖ Flux reconciliation complete"
    preconditions:
      - which flux

  suspend-namespace:
    desc: Suspend all Flux Kustomizations in a namespace [NAMESPACE=required]
    summary: |
      Suspend all Flux Kustomizations in a namespace to stop reconciliation.
    requires:
      vars: [NAMESPACE]
    cmds:
      - |
        echo "Suspending all Kustomizations in namespace: {{.NAMESPACE}}"
        kubectl get kustomizations -n {{.NAMESPACE}} --no-headers -o custom-columns=":metadata.name" | \
        xargs -I {} flux suspend kustomization {} -n {{.NAMESPACE}}
      - echo "‚úÖ All Kustomizations in {{.NAMESPACE}} namespace suspended"
    preconditions:
      - which flux kubectl

  resume-namespace:
    desc: Resume all Flux Kustomizations in a namespace [NAMESPACE=required]
    summary: |
      Resume all Flux Kustomizations in a namespace to restore reconciliation.
    requires:
      vars: [NAMESPACE]
    cmds:
      - |
        echo "Resuming all Kustomizations in namespace: {{.NAMESPACE}}"
        kubectl get kustomizations -n {{.NAMESPACE}} --no-headers -o custom-columns=":metadata.name" | \
        xargs -I {} flux resume kustomization {} -n {{.NAMESPACE}}
      - echo "‚úÖ All Kustomizations in {{.NAMESPACE}} namespace resumed"
    preconditions:
      - which flux kubectl

  nuke-namespace:
    desc: Nuclear option - suspend kustomizations AND delete all pods [NAMESPACE=required]
    summary: |
      Nuclear option: Suspend all Flux Kustomizations AND delete all pods in namespace.

      Warning: This completely stops and cleans a namespace.
    requires:
      vars: [NAMESPACE]
    cmds:
      - |
        echo "üö® NUCLEAR OPTION: This will suspend all Kustomizations AND delete all pods in: {{.NAMESPACE}}"
        read -p "Are you absolutely sure? (type 'NUKE' to confirm): " confirm
        if [[ $confirm == "NUKE" ]]; then
          if ! task k8s:suspend-namespace NAMESPACE={{.NAMESPACE}}; then
            echo "‚ùå Failed to suspend namespace"
            exit 1
          fi
          sleep 2
          if ! kubectl delete pods -n {{.NAMESPACE}} --all --grace-period=0 --force; then
            echo "‚ùå Failed to delete pods"
            exit 1
          fi
          echo "üí• Namespace {{.NAMESPACE}} has been nuked"
        else
          echo "‚ùå Operation cancelled"
        fi
    preconditions:
      - which flux kubectl

  cleanse-pods:
    desc: Cleanse pods with a Failed/Pending/Succeeded phase
    cmds:
      - for:
          matrix:
            PHASE: [Failed, Pending, Succeeded]
        cmd: kubectl delete pods --all-namespaces --field-selector status.phase={{.ITEM.PHASE}} --ignore-not-found=true
    preconditions:
      - which kubectl

  delete-namespace-pods:
    desc: Delete all pods in a namespace [NAMESPACE=required]
    summary: |
      Delete all pods in a namespace to free resources immediately.

      Warning: This forcefully deletes all pods in the namespace.
    requires:
      vars: [NAMESPACE]
    cmds:
      - |
        echo "‚ö†Ô∏è  WARNING: This will delete ALL pods in namespace: {{.NAMESPACE}}"
        read -p "Are you sure? (y/N): " confirm
        if [[ $confirm == [yY] ]]; then
          kubectl delete pods -n {{.NAMESPACE}} --all --grace-period=0 --force
          echo "‚úÖ All pods in {{.NAMESPACE}} namespace deleted"
        else
          echo "‚ùå Operation cancelled"
        fi

  delete-deployment:
    desc: Delete a specific deployment and its pods [DEPLOYMENT=required NAMESPACE=required]
    summary: |
      Delete a specific deployment and all its pods.
    requires:
      vars: [DEPLOYMENT, NAMESPACE]
    cmds:
      - |
        echo "Deleting deployment: {{.DEPLOYMENT}} in namespace: {{.NAMESPACE}}"
        kubectl delete deployment {{.DEPLOYMENT}} -n {{.NAMESPACE}} --grace-period=0 --force
        echo "‚úÖ Deployment {{.DEPLOYMENT}} deleted from {{.NAMESPACE}} namespace"

  scale-deployment:
    desc: Scale a deployment to specific replica count [DEPLOYMENT=required NAMESPACE=required REPLICAS=required]
    requires:
      vars: [DEPLOYMENT, NAMESPACE, REPLICAS]
    cmds:
      - |
        echo "Scaling deployment {{.DEPLOYMENT}} in {{.NAMESPACE}} to {{.REPLICAS}} replicas"
        kubectl scale deployment {{.DEPLOYMENT}} -n {{.NAMESPACE}} --replicas={{.REPLICAS}}
        echo "‚úÖ Deployment {{.DEPLOYMENT}} scaled to {{.REPLICAS}} replicas"

  restart-deployment:
    desc: Restart a deployment by rolling it out [DEPLOYMENT=required NAMESPACE=required]
    requires:
      vars: [DEPLOYMENT, NAMESPACE]
    cmds:
      - |
        echo "Restarting deployment: {{.DEPLOYMENT}} in namespace: {{.NAMESPACE}}"
        kubectl rollout restart deployment {{.DEPLOYMENT}} -n {{.NAMESPACE}}
        echo "‚úÖ Deployment {{.DEPLOYMENT}} restart initiated"
        kubectl rollout status deployment {{.DEPLOYMENT}} -n {{.NAMESPACE}} --timeout=300s

  check-resource-pressure:
    desc: Check cluster resource pressure and identify constraining pods
    cmds:
      - |
        PROBLEMATIC_STATUSES="(Pending|CrashLoopBackOff|Error|ImagePullBackOff)"

        echo "üìä Cluster Resource Pressure Analysis"
        echo "======================================"
        echo ""
        echo "üñ•Ô∏è  Node Resource Usage:"
        if ! kubectl top nodes 2>/dev/null; then
          echo "‚ùå Failed to get node metrics"
          exit 1
        fi
        echo ""
        echo "üî• Top 10 CPU Consuming Pods:"
        cpu_output=$(kubectl top pods -A --sort-by=cpu 2>/dev/null)
        if [[ $? -eq 0 && -n "$cpu_output" ]]; then
          echo "$cpu_output" | head -11 || true
        else
          echo "‚ùå Failed to get pod CPU metrics"
          exit 1
        fi
        echo ""
        echo "üíæ Top 10 Memory Consuming Pods:"
        memory_output=$(kubectl top pods -A --sort-by=memory 2>/dev/null)
        if [[ $? -eq 0 && -n "$memory_output" ]]; then
          echo "$memory_output" | head -11 || true
        else
          echo "‚ùå Failed to get pod memory metrics"
          exit 1
        fi
        echo ""
        echo "‚ö†Ô∏è  Problematic Pods:"
        problematic_pods=$(kubectl get pods -A 2>/dev/null | grep -E "$PROBLEMATIC_STATUSES" || true)
        if [[ -n "$problematic_pods" ]]; then
          echo "$problematic_pods"
        else
          echo "‚úÖ No problematic pods found"
        fi
        echo ""
        echo "üìã Summary:"
        total_pods=$(kubectl get pods -A --no-headers 2>/dev/null | wc -l || echo "unknown")
        running_pods=$(kubectl get pods -A --no-headers 2>/dev/null | grep -c "Running" || echo "unknown")
        echo "Total pods: $total_pods | Running: $running_pods"

  force-pod-delete:
    desc: Force delete a stuck pod [POD=required NAMESPACE=required]
    requires:
      vars: [POD, NAMESPACE]
    cmds:
      - |
        echo "Force deleting pod: {{.POD}} in namespace: {{.NAMESPACE}}"
        kubectl delete pod {{.POD}} -n {{.NAMESPACE}} --grace-period=0 --force
        echo "‚úÖ Pod {{.POD}} force deleted"

  nuke-app:
    desc: Nuclear option - completely purge an app and let Flux rebuild [APP=required NAMESPACE=required]
    requires:
      vars: [APP, NAMESPACE]
    cmds:
      - |
        set -euo pipefail

        safe_delete() {
          local resource_type="$1"
          local namespace="$2"
          local app="$3"
          local label_selector="app.kubernetes.io/name=$app"

          echo "Deleting $resource_type with label $label_selector..."
          kubectl delete "$resource_type" -n "$namespace" -l "$label_selector" --ignore-not-found=true || echo "‚ö†Ô∏è  Failed to delete labeled $resource_type"

          echo "Deleting additional $resource_type matching '$app'..."
          set +e
          local raw_output
          raw_output=$(kubectl get "$resource_type" -n "$namespace" --no-headers 2>/dev/null | grep "$app" 2>/dev/null)
          local grep_exit_code=$?
          set -e

          if [[ "$grep_exit_code" -eq 0 ]]; then
            echo "$raw_output" | awk '{print $1}' | while read -r res_name; do
              kubectl delete "$resource_type" -n "$namespace" "$res_name" || echo "‚ö†Ô∏è  Failed to delete $resource_type: $res_name"
            done
          else
            echo "No additional $resource_type found matching '$app'"
          fi
        }

        echo "üö® NUCLEAR APP DESTRUCTION: {{.APP}} in namespace {{.NAMESPACE}}"
        echo "This will completely destroy and rebuild the application!"
        read -p "Type 'NUKE {{.APP}}' to confirm: " confirm
        if [[ $confirm != "NUKE {{.APP}}" ]]; then
          echo "‚ùå Operation cancelled"
          exit 1
        fi

        echo "üîÑ Step 1: Suspending Flux HelmRelease..."
        if kubectl get helmrelease {{.APP}} -n {{.NAMESPACE}} >/dev/null 2>&1; then
          flux suspend helmrelease {{.APP}} -n {{.NAMESPACE}}
          echo "‚úÖ HelmRelease {{.APP}} suspended"
        else
          echo "‚ö†Ô∏è  No HelmRelease found for {{.APP}}"
        fi

        echo "üóëÔ∏è  Step 2: Deleting Helm release..."
        if helm list -n {{.NAMESPACE}} | grep -q {{.APP}}; then
          helm uninstall {{.APP}} -n {{.NAMESPACE}}
          echo "‚úÖ Helm release {{.APP}} deleted"
        else
          echo "‚ö†Ô∏è  No Helm release found for {{.APP}}"
        fi

        echo "üîë Step 3: Deleting app-related secrets..."
        safe_delete "secrets" "{{.NAMESPACE}}" "{{.APP}}"

        echo "üìã Step 4: Deleting app-related configmaps..."
        safe_delete "configmaps" "{{.NAMESPACE}}" "{{.APP}}"

        echo "‚ò†Ô∏è  Step 5: Force deleting all app resources..."
        kubectl delete pods -n {{.NAMESPACE}} -l app.kubernetes.io/name={{.APP}} --grace-period=0 --force --ignore-not-found=true || true
        kubectl delete replicasets -n {{.NAMESPACE}} -l app.kubernetes.io/name={{.APP}} --grace-period=0 --force --ignore-not-found=true || true
        kubectl delete deployments -n {{.NAMESPACE}} -l app.kubernetes.io/name={{.APP}} --grace-period=0 --force --ignore-not-found=true || true
        safe_delete "hpa" "{{.NAMESPACE}}" "{{.APP}}"

        echo "‚è≥ Waiting for resource cleanup..."
        sleep 10

        echo "üíæ Step 6: Deleting app-related PVCs..."
        safe_delete "pvc" "{{.NAMESPACE}}" "{{.APP}}"

        echo "‚è≥ Step 7: Waiting for cleanup..."
        sleep 5

        echo "üîÑ Step 8: Resuming Flux HelmRelease for clean rebuild..."
        if kubectl get helmrelease {{.APP}} -n {{.NAMESPACE}} >/dev/null 2>&1; then
          flux resume helmrelease {{.APP}} -n {{.NAMESPACE}} || echo "‚ö†Ô∏è  Failed to resume HelmRelease"
          flux reconcile helmrelease {{.APP}} -n {{.NAMESPACE}} || echo "‚ö†Ô∏è  Failed to reconcile HelmRelease"
        else
          echo "‚ö†Ô∏è  No HelmRelease to resume - check Git configuration"
        fi

        echo "üí• App {{.APP}} has been completely nuked and rebuild initiated"
    preconditions:
      - which flux helm kubectl
